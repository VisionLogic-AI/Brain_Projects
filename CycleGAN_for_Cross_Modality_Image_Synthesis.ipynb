{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN for Cross_Modality Image Synthesis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWlxEsn1jK0pXKzVHEQF4v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VisionLogic-AI/Brain_Projects/blob/master/CycleGAN_for_Cross_Modality_Image_Synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM2XrOrjUS0P",
        "colab_type": "text"
      },
      "source": [
        "#CycleGAN\n",
        "What is CycleGAN?\n",
        "CycleGAN is an unsupervised image to image architecture known for it's capacity for learning appings between classes of images without requiring paired data, making it something of a \"universal image translator\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r_FO9CdT3sR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2916cc86-bb87-4259-f8e4-7a7d59181e1d"
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/requirements-gpu.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-20 18:10:25--  https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/requirements-gpu.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90 [text/plain]\n",
            "Saving to: ‘requirements-gpu.txt’\n",
            "\n",
            "\rrequirements-gpu.tx   0%[                    ]       0  --.-KB/s               \rrequirements-gpu.tx 100%[===================>]      90  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-20 18:10:25 (5.53 MB/s) - ‘requirements-gpu.txt’ saved [90/90]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jmx9gohXrKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "83070163-4674-4c9b-dd74-1804c2600d85"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfux9Ju5VTB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b34e6fe-e87c-43c8-a5d1-7ebf23b96b93"
      },
      "source": [
        "!pip install -r requirements-gpu.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git (from -r requirements-gpu.txt (line 2))\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-r9svdly8\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-r9svdly8\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 2))\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 1)) (2.3.1)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 4)) (3.2.1)\n",
            "Requirement already satisfied: progress in /usr/local/lib/python3.6/dist-packages (from -r requirements-gpu.txt (line 5)) (1.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (1.18.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements-gpu.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (4.7.4)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (5.2.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (4.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements-gpu.txt (line 3)) (7.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-gpu.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-gpu.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-gpu.txt (line 4)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements-gpu.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (4.6.3)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (4.3.3)\n",
            "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (5.3.4)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (19.0.1)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->-r requirements-gpu.txt (line 3)) (0.8.3)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->-r requirements-gpu.txt (line 3)) (4.5.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->-r requirements-gpu.txt (line 3)) (2.11.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->-r requirements-gpu.txt (line 3)) (5.0.6)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->-r requirements-gpu.txt (line 3)) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->-r requirements-gpu.txt (line 3)) (1.0.18)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (3.1.5)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->-r requirements-gpu.txt (line 3)) (3.5.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter->-r requirements-gpu.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->-r requirements-gpu.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->-r requirements-gpu.txt (line 3)) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter->-r requirements-gpu.txt (line 3)) (2.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->-r requirements-gpu.txt (line 3)) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->-r requirements-gpu.txt (line 3)) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->-r requirements-gpu.txt (line 3)) (46.3.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->-r requirements-gpu.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->-r requirements-gpu.txt (line 3)) (0.1.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (20.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->-r requirements-gpu.txt (line 3)) (0.5.1)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101064 sha256=9fb04536ef1c98b48964ea8223f74014eef633d608c6277177ac82a7d06e4690\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c9ds07k_/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHosBuv2ViW_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9dec9126-cc8f-4e73-9778-5a4f5c362446"
      },
      "source": [
        "from keras.layers import Layer, Input, Dropout, Conv2D, Activation, add, UpSampling2D\n",
        "from keras.layers import Conv2DTranspose, Flatten\n",
        "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization, InputSpec\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.engine.topology import Network\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from progress.bar import Bar\n",
        "import datetime\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import keras.backend as K"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sim_b7QRW7VP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4c274e38-cc6b-4cab-ae3d-b9b2c47edcec"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/brainhack101/IntroDl/master/notebooks/2019/Eklund/helper_funcs.py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-20 13:57:03--  https://raw.githubusercontent.com/brainhack101/IntroDl/master/notebooks/2019/Eklund/helper_funcs.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13105 (13K) [text/plain]\n",
            "Saving to: ‘helper_funcs.py.2’\n",
            "\n",
            "\rhelper_funcs.py.2     0%[                    ]       0  --.-KB/s               \rhelper_funcs.py.2   100%[===================>]  12.80K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-05-20 13:57:03 (73.3 MB/s) - ‘helper_funcs.py.2’ saved [13105/13105]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUCgA9ZNX87r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from helper_funcs import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBLPt3epYCHb",
        "colab_type": "text"
      },
      "source": [
        "If you have multiple GPU's, you can select a songle one of them by setting te visible CUDA device 0,1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhEaxYw3YNwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_DEVICE_ORDER']= 'PCI_BUS_ID'\n",
        "os.environ['CUDA_VISIBLE_DEVICES']= '0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8lsEJUyYaSU",
        "colab_type": "text"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GTdHACeYezK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f88d924d-a9de-4492-db90-51b9483bc5ae"
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/data.zip\n",
        "!unzip -uo data.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-20 14:19:23--  https://raw.githubusercontent.com/brainhack101/IntroDL/master/notebooks/2019/Eklund/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 531636 (519K) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 519.18K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-05-20 14:19:24 (7.56 MB/s) - ‘data.zip’ saved [531636/531636]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/malefemale/\n",
            "   creating: data/malefemale/testA/\n",
            "  inflating: data/malefemale/testA/im163.png  \n",
            "  inflating: data/malefemale/testA/im188.png  \n",
            "  inflating: data/malefemale/testA/im189.png  \n",
            "  inflating: data/malefemale/testA/im175.png  \n",
            "  inflating: data/malefemale/testA/im171.png  \n",
            "  inflating: data/malefemale/testA/im170.png  \n",
            "  inflating: data/malefemale/testA/im194.png  \n",
            "  inflating: data/malefemale/testA/im196.png  \n",
            "  inflating: data/malefemale/testA/im168.png  \n",
            "  inflating: data/malefemale/testA/im178.png  \n",
            "  inflating: data/malefemale/testA/im193.png  \n",
            "  inflating: data/malefemale/testA/im179.png  \n",
            "  inflating: data/malefemale/testA/im190.png  \n",
            "   creating: data/malefemale/trainB/\n",
            "  inflating: data/malefemale/trainB/im87.png  \n",
            "  inflating: data/malefemale/trainB/im93.png  \n",
            "  inflating: data/malefemale/trainB/im44.png  \n",
            "  inflating: data/malefemale/trainB/im50.png  \n",
            "  inflating: data/malefemale/trainB/im79.png  \n",
            "  inflating: data/malefemale/trainB/im92.png  \n",
            "  inflating: data/malefemale/trainB/im86.png  \n",
            "  inflating: data/malefemale/trainB/im90.png  \n",
            "  inflating: data/malefemale/trainB/im47.png  \n",
            "  inflating: data/malefemale/trainB/im85.png  \n",
            "  inflating: data/malefemale/trainB/im91.png  \n",
            "  inflating: data/malefemale/trainB/im81.png  \n",
            "  inflating: data/malefemale/trainB/im56.png  \n",
            "  inflating: data/malefemale/trainB/im57.png  \n",
            "  inflating: data/malefemale/trainB/im80.png  \n",
            "  inflating: data/malefemale/trainB/im94.png  \n",
            "  inflating: data/malefemale/trainB/im82.png  \n",
            "  inflating: data/malefemale/trainB/im69.png  \n",
            "  inflating: data/malefemale/trainB/im54.png  \n",
            "  inflating: data/malefemale/trainB/im40.png  \n",
            "  inflating: data/malefemale/trainB/im68.png  \n",
            "  inflating: data/malefemale/trainB/im27.png  \n",
            "  inflating: data/malefemale/trainB/im33.png  \n",
            "  inflating: data/malefemale/trainB/im114.png  \n",
            "  inflating: data/malefemale/trainB/im115.png  \n",
            "  inflating: data/malefemale/trainB/im101.png  \n",
            "  inflating: data/malefemale/trainB/im6.png  \n",
            "  inflating: data/malefemale/trainB/im32.png  \n",
            "  inflating: data/malefemale/trainB/im30.png  \n",
            "  inflating: data/malefemale/trainB/im24.png  \n",
            "  inflating: data/malefemale/trainB/im18.png  \n",
            "  inflating: data/malefemale/trainB/im117.png  \n",
            "  inflating: data/malefemale/trainB/im103.png  \n",
            "  inflating: data/malefemale/trainB/im116.png  \n",
            "  inflating: data/malefemale/trainB/im19.png  \n",
            "  inflating: data/malefemale/trainB/im31.png  \n",
            "  inflating: data/malefemale/trainB/im35.png  \n",
            "  inflating: data/malefemale/trainB/im1.png  \n",
            "  inflating: data/malefemale/trainB/im21.png  \n",
            "  inflating: data/malefemale/trainB/im34.png  \n",
            "  inflating: data/malefemale/trainB/im22.png  \n",
            "  inflating: data/malefemale/trainB/im2.png  \n",
            "  inflating: data/malefemale/trainB/im104.png  \n",
            "  inflating: data/malefemale/trainB/im138.png  \n",
            "  inflating: data/malefemale/trainB/im3.png  \n",
            "  inflating: data/malefemale/trainB/im37.png  \n",
            "  inflating: data/malefemale/trainB/im23.png  \n",
            "  inflating: data/malefemale/trainB/im135.png  \n",
            "  inflating: data/malefemale/trainB/im109.png  \n",
            "  inflating: data/malefemale/trainB/im108.png  \n",
            "  inflating: data/malefemale/trainB/im134.png  \n",
            "  inflating: data/malefemale/trainB/im136.png  \n",
            "  inflating: data/malefemale/trainB/im123.png  \n",
            "  inflating: data/malefemale/trainB/im127.png  \n",
            "  inflating: data/malefemale/trainB/im9.png  \n",
            "  inflating: data/malefemale/trainB/im17.png  \n",
            "  inflating: data/malefemale/trainB/im118.png  \n",
            "  inflating: data/malefemale/trainB/im124.png  \n",
            "  inflating: data/malefemale/trainB/im125.png  \n",
            "  inflating: data/malefemale/trainB/im119.png  \n",
            "  inflating: data/malefemale/trainB/im16.png  \n",
            "  inflating: data/malefemale/trainB/im65.png  \n",
            "  inflating: data/malefemale/trainB/im71.png  \n",
            "  inflating: data/malefemale/trainB/im59.png  \n",
            "  inflating: data/malefemale/trainB/im143.png  \n",
            "  inflating: data/malefemale/trainB/im58.png  \n",
            "  inflating: data/malefemale/trainB/im70.png  \n",
            "  inflating: data/malefemale/trainB/im99.png  \n",
            "  inflating: data/malefemale/trainB/im67.png  \n",
            "  inflating: data/malefemale/trainB/im63.png  \n",
            "  inflating: data/malefemale/trainB/im150.png  \n",
            "  inflating: data/malefemale/trainB/im144.png  \n",
            "  inflating: data/malefemale/trainB/im151.png  \n",
            "  inflating: data/malefemale/trainB/im62.png  \n",
            "  inflating: data/malefemale/trainB/im76.png  \n",
            "  inflating: data/malefemale/trainB/im89.png  \n",
            "  inflating: data/malefemale/trainB/im60.png  \n",
            "  inflating: data/malefemale/trainB/im75.png  \n",
            "  inflating: data/malefemale/trainB/im61.png  \n",
            "  inflating: data/malefemale/trainB/im49.png  \n",
            "   creating: data/malefemale/testB/\n",
            "  inflating: data/malefemale/testB/im177.png  \n",
            "  inflating: data/malefemale/testB/im176.png  \n",
            "  inflating: data/malefemale/testB/im162.png  \n",
            "  inflating: data/malefemale/testB/im200.png  \n",
            "  inflating: data/malefemale/testB/im174.png  \n",
            "  inflating: data/malefemale/testB/im165.png  \n",
            "  inflating: data/malefemale/testB/im164.png  \n",
            "  inflating: data/malefemale/testB/im166.png  \n",
            "  inflating: data/malefemale/testB/im172.png  \n",
            "  inflating: data/malefemale/testB/im199.png  \n",
            "  inflating: data/malefemale/testB/im198.png  \n",
            "  inflating: data/malefemale/testB/im173.png  \n",
            "  inflating: data/malefemale/testB/im167.png  \n",
            "  inflating: data/malefemale/testB/im156.png  \n",
            "  inflating: data/malefemale/testB/im181.png  \n",
            "  inflating: data/malefemale/testB/im195.png  \n",
            "  inflating: data/malefemale/testB/im180.png  \n",
            "  inflating: data/malefemale/testB/im169.png  \n",
            "  inflating: data/malefemale/testB/im182.png  \n",
            "  inflating: data/malefemale/testB/im183.png  \n",
            "  inflating: data/malefemale/testB/im197.png  \n",
            "  inflating: data/malefemale/testB/im187.png  \n",
            "  inflating: data/malefemale/testB/im186.png  \n",
            "  inflating: data/malefemale/testB/im192.png  \n",
            "  inflating: data/malefemale/testB/im184.png  \n",
            "  inflating: data/malefemale/testB/im191.png  \n",
            "  inflating: data/malefemale/testB/im185.png  \n",
            "   creating: data/malefemale/trainA/\n",
            "  inflating: data/malefemale/trainA/im78.png  \n",
            "  inflating: data/malefemale/trainA/im51.png  \n",
            "  inflating: data/malefemale/trainA/im45.png  \n",
            "  inflating: data/malefemale/trainA/im84.png  \n",
            "  inflating: data/malefemale/trainA/im53.png  \n",
            "  inflating: data/malefemale/trainA/im160.png  \n",
            "  inflating: data/malefemale/trainA/im148.png  \n",
            "  inflating: data/malefemale/trainA/im149.png  \n",
            "  inflating: data/malefemale/trainA/im161.png  \n",
            "  inflating: data/malefemale/trainA/im46.png  \n",
            "  inflating: data/malefemale/trainA/im52.png  \n",
            "  inflating: data/malefemale/trainA/im95.png  \n",
            "  inflating: data/malefemale/trainA/im42.png  \n",
            "  inflating: data/malefemale/trainA/im159.png  \n",
            "  inflating: data/malefemale/trainA/im158.png  \n",
            "  inflating: data/malefemale/trainA/im43.png  \n",
            "  inflating: data/malefemale/trainA/im96.png  \n",
            "  inflating: data/malefemale/trainA/im41.png  \n",
            "  inflating: data/malefemale/trainA/im55.png  \n",
            "  inflating: data/malefemale/trainA/im97.png  \n",
            "  inflating: data/malefemale/trainA/im83.png  \n",
            "  inflating: data/malefemale/trainA/im7.png  \n",
            "  inflating: data/malefemale/trainA/im100.png  \n",
            "  inflating: data/malefemale/trainA/im128.png  \n",
            "  inflating: data/malefemale/trainA/im129.png  \n",
            "  inflating: data/malefemale/trainA/im26.png  \n",
            "  inflating: data/malefemale/trainA/im4.png  \n",
            "  inflating: data/malefemale/trainA/im102.png  \n",
            "  inflating: data/malefemale/trainA/im25.png  \n",
            "  inflating: data/malefemale/trainA/im5.png  \n",
            "  inflating: data/malefemale/trainA/im112.png  \n",
            "  inflating: data/malefemale/trainA/im106.png  \n",
            "  inflating: data/malefemale/trainA/im107.png  \n",
            "  inflating: data/malefemale/trainA/im113.png  \n",
            "  inflating: data/malefemale/trainA/im20.png  \n",
            "  inflating: data/malefemale/trainA/im36.png  \n",
            "  inflating: data/malefemale/trainA/im139.png  \n",
            "  inflating: data/malefemale/trainA/im105.png  \n",
            "  inflating: data/malefemale/trainA/im111.png  \n",
            "  inflating: data/malefemale/trainA/im110.png  \n",
            "  inflating: data/malefemale/trainA/im12.png  \n",
            "  inflating: data/malefemale/trainA/im121.png  \n",
            "  inflating: data/malefemale/trainA/im120.png  \n",
            "  inflating: data/malefemale/trainA/im13.png  \n",
            "  inflating: data/malefemale/trainA/im11.png  \n",
            "  inflating: data/malefemale/trainA/im39.png  \n",
            "  inflating: data/malefemale/trainA/im122.png  \n",
            "  inflating: data/malefemale/trainA/im137.png  \n",
            "  inflating: data/malefemale/trainA/im38.png  \n",
            "  inflating: data/malefemale/trainA/im10.png  \n",
            "  inflating: data/malefemale/trainA/im28.png  \n",
            "  inflating: data/malefemale/trainA/im8.png  \n",
            "  inflating: data/malefemale/trainA/im14.png  \n",
            "  inflating: data/malefemale/trainA/im133.png  \n",
            "  inflating: data/malefemale/trainA/im126.png  \n",
            "  inflating: data/malefemale/trainA/im132.png  \n",
            "  inflating: data/malefemale/trainA/im15.png  \n",
            "  inflating: data/malefemale/trainA/im29.png  \n",
            "  inflating: data/malefemale/trainA/im130.png  \n",
            "  inflating: data/malefemale/trainA/im131.png  \n",
            "  inflating: data/malefemale/trainA/im142.png  \n",
            "  inflating: data/malefemale/trainA/im157.png  \n",
            "  inflating: data/malefemale/trainA/im64.png  \n",
            "  inflating: data/malefemale/trainA/im72.png  \n",
            "  inflating: data/malefemale/trainA/im66.png  \n",
            "  inflating: data/malefemale/trainA/im155.png  \n",
            "  inflating: data/malefemale/trainA/im141.png  \n",
            "  inflating: data/malefemale/trainA/im140.png  \n",
            "  inflating: data/malefemale/trainA/im154.png  \n",
            "  inflating: data/malefemale/trainA/im73.png  \n",
            "  inflating: data/malefemale/trainA/im98.png  \n",
            "  inflating: data/malefemale/trainA/im88.png  \n",
            "  inflating: data/malefemale/trainA/im77.png  \n",
            "  inflating: data/malefemale/trainA/im145.png  \n",
            "  inflating: data/malefemale/trainA/im48.png  \n",
            "  inflating: data/malefemale/trainA/im74.png  \n",
            "  inflating: data/malefemale/trainA/im147.png  \n",
            "  inflating: data/malefemale/trainA/im153.png  \n",
            "  inflating: data/malefemale/trainA/im152.png  \n",
            "  inflating: data/malefemale/trainA/im146.png  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLB0hlMvdB0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_folder= 'malefemale'\n",
        "data= load_data(subfolder= image_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6smIujEdJpv",
        "colab_type": "text"
      },
      "source": [
        "#Model Parameters\n",
        "This CycleGAN implementation allows a alot of freedom on both the training parameters and the network and network architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z48ug3uXdX27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt= {}\n",
        "\n",
        "#Data\n",
        "opt['channels']= data['nr_of_channels']\n",
        "opt['img_shape']= data['images_size'] + (opt['channels'], )\n",
        "print('Image shape:', opt['img_shape'])\n",
        "\n",
        "opt['A_Train']= data['trainA_images']\n",
        "opt['B_Train']= data['trainB_images']\n",
        "opt['A_test']= data['testA_images']\n",
        "opt['B_test']= data['testB_images']\n",
        "opt['testA_images_names']= data['testA_image_names']\n",
        "opt['testB_images_names']= data['testB_image_names']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSMGvppteOYX",
        "colab_type": "text"
      },
      "source": [
        "CycleGAN can be used on both paired and unpaired data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXDu_1CXeVNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt['paired_data']= False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RULBv9LgedAR",
        "colab_type": "text"
      },
      "source": [
        "Training Parameters\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEAzO_Qke4rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training Parameters\n",
        "opt['lambda_ABA']= 10.0   #cyclic loss weight A_2_B\n",
        "opt['lambda_BAB']= 10.0  #cyclic loss weight B_2_A\n",
        "opt['lmbda_adversarial']= 10.0    #weight for los from discriminator guess on synthetic images\n",
        "opt['learning_rate_D']= 2e-4\n",
        "opt['learning_rate_G']= 2e-4\n",
        "opt['generator_iterations']= 3   #number of generator training iterations in each trainining loop\n",
        "opt['discriminator_iterations']= 1   #number of discriminator traijning iterations in each training loop\n",
        "opt['synthetic_pool_size']= 50   #size of image pools used for training the discriminators\n",
        "opt['beta_1']= 0.5  #adam parameter\n",
        "opt['beta_2']= 0.999  #adam parameter\n",
        "opt['batch_size']= 10    #number of images per batch\n",
        "opt['epochs']= 200    #choose multiples of 20 since the models are saved each 20th epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-laBQ8lh2W0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Output parameters\n",
        "opt['save_models']= True   #save or not the generator and discriminator models\n",
        "opt['save-training_img']= True   #save or not example training results or only tmp.png\n",
        "opt['save_training_img_interval']= 1   #number of epoch between saves of intermediate training results\n",
        "opt['self.tmp_img_update_frequency']= 3   #number of batches between updates of tmp.pn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFCFCwo2idTK",
        "colab_type": "text"
      },
      "source": [
        "Architecture Parameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUwG6qvJjUUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Architecture parameters\n",
        "opt['use_instance_normalization']= True    #use instance normalization or batch normalization\n",
        "opt['use_dropout']= False  #dropout in residual blocks\n",
        "opt['use_bias']= True   #use bias\n",
        "opt['use_linear_decay']= True   #linear decay of learning rate, for both discriminators and generators\n",
        "opt['decay_epoch']= 101   #the epoch where the linear decay of the learning rates start\n",
        "opt['use_patchgun']= True   #PatchGAN - if false the discriminators learning rate should be decreased\n",
        "opt['use_resize_convolution']= False    #resize convolution - instead of transpose convolution in deconvolution layers\n",
        "opt['discriminator_sigmoid']= True    #add a final sigmoid activation to the disctiminator\n",
        "\n",
        "#tweaks\n",
        "opt['REAL_LABEL']= 1.0  #use e.g 0.9 to avoid training the discriminators to zero loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq-xV8mrkwX6",
        "colab_type": "text"
      },
      "source": [
        "#Model Architecture\n",
        "Layer Blocks\n",
        "These are the individual layer blocks that are used to build the generators and discriminator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZw7FB-Tk_DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Discriminator Layers\n",
        "def ck(model, opt, x, k, use_normalization, use_bias):\n",
        "  x= Conv2D(filters= k, kernel_size=4, strides=2, padding= 'same', use_bias= use_bias)(x)\n",
        "  if use_normalization:\n",
        "    x= model['normalization'](axis= 3, center= True, epsilon= 1e-5)(x, training= True)\n",
        "    x= LeakyReLU(alpha= 0.2)(x)\n",
        "    return x\n",
        "\n",
        "#first generator layer\n",
        "def c7AK(model, opt, x, k):\n",
        "  x= Conv2D(filters= k, kernel_size= 7, strides= 1, padding= 'valid', use_bias= opt['use_bias'])(x)\n",
        "  x= model['normalization'](axis= 3, center= True, epsilon= 1e-5)(x, training= True)\n",
        "  x= Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "#Downsampling\n",
        "def dk(model, opt, x, k): \n",
        "  x= Conv2D(filters= k, kernel_size=3, strides=2, padding='same', use_bias= opt['use_bias'])(x)\n",
        "  x= model['noralization'](axis= 3, centers = True, epsilon= 1e-5)(x, training= True)\n",
        "  x= Activation('relu')(x)\n",
        "  return x\n",
        "\n",
        "#Residual block\n",
        "def Rk(model, opt, x0):\n",
        "  k= int(x0.shape[-1])\n",
        "\n",
        "  #first layer\n",
        "  x= ReflectionPadding2D((1,1))(x0)\n",
        "  x= Conv2D(filters= k, kernel_size 3, strides=1, padding= 'valid', use_bias= opt['use_bias'])(x)\n",
        "  x= model['noramlization'](axis=3, center= True, epsilon= 1e-5)(x, training= True)\n",
        "  x= Activation('relu')(x)\n",
        "  if opt['use_dropout']:\n",
        "    x= Dropout(0.5)(x)\n",
        "\n",
        "  #second layer\n",
        "  x= ReflectionPadding2D((1,1))(x)\n",
        "  x= Conv2D(filters= k, kernel_size= 3, strides= 1, padding= 'valid', use_bias= opt['use_bias'])(x)\n",
        "  x = model['normalization'](axis=3, center= True, epsilon= 1e-5)(x, training= True)\n",
        "  #Merge\n",
        "  x= add([x, x0])\n",
        "\n",
        "  return x\n",
        "\n",
        "#UpSampling\n",
        "def uk(model, opt, x, k):\n",
        "  # up sampling followed by 1x1 convolution <=> fractional - strided 1/2\n",
        "  if opt['use_resize_convolution']:\n",
        "    x= UpSampling2D(size= (2,2))(x)    #nearest neighbor upsampling\n",
        "    x= ReflectionPadding2D((1,1))(x)\n",
        "    x= Conv2D(filters=k, kernel_size= 3, strides=1, padding= 'valid', use_bias= opt['use_bias'])(x)\n",
        "  else:\n",
        "    x= Conv2DTranspose(filters=k, kernel_size=3, strides=2, padding= 'same', use_bias= opt['use_bias'])(x)\n",
        "  x= model['normalization'](axis= 3, center= True, epsilon= 1e-5)(x, training= True)\n",
        "  x= Activation('relu')(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "232fi6IhsBlS",
        "colab_type": "text"
      },
      "source": [
        "Architecture Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCKz3VohsErD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator(model, opt, name= None):\n",
        "  #Input\n",
        "  input_img= Input(shape= opt,['img_shape'])\n",
        "\n",
        "  #Layers 1-4\n",
        "  x= ck(model, opt, input_img, 64, False, True)    #instance normalization is not used for this layer\n",
        "  x= ck(model, opt, x, 128, True, opt['use_bias'])\n",
        "  x= ck(model, opt, x, 256, True, opt['use_bias'])\n",
        "  x= ck(model, opt, x, 512, True, opt['use_bias'])\n",
        "\n",
        "  #Layer 5: Output\n",
        "  if opt['use_patchgan']:\n",
        "    x= Conv2D(filters= 1, kernel_size= 4, strides=1, padding= 'same', use_bias= True)(x)\n",
        "  else:\n",
        "    x= Flatten()(x)\n",
        "    x= Dense(1)(x)\n",
        "\n",
        "  if opt['discriminator_sigmoid']:\n",
        "    x= Activation('sigmoid')(x)\n",
        "\n",
        "  return Model(inputs= input_img, outputs= x, name=name)\n",
        "\n",
        "def build_generator(model, opt, name= None):\n",
        "  #layer 1: Input\n",
        "  input_img= Input(shape= opt['img_shape'])\n",
        "  x= ReflectionPadding2D((3,3))(input_img)\n",
        "  x= c7AK(model, opt, x, 32)\n",
        "\n",
        "  #Layer 2-3: Downsampling\n",
        "  x= dk(model, opt, x, 64)\n",
        "  x= df(model, opt, x, 128)\n",
        "\n",
        "  #Layers 4-12: Residual blocks\n",
        "  for _ in range(4, 13):\n",
        "    x= Rk(model, opt, x)\n",
        "\n",
        "  #layer 13:14: UpSampling\n",
        "  x= uk(model, opt, x, 64)\n",
        "  x= uk(model, opt, x, 32)\n",
        "\n",
        "  #layer 15: Output\n",
        "  x= ReflectionPadding2D((3,3))(x)\n",
        "  x= Conv2D(opt['channels'], kernel_size= 7, strides= 1, padding= 'valid', use_bias= True)(x)\n",
        "  x= Activation('tanh')(x)\n",
        "\n",
        "  return Model(inputs= input_img, outputs= x, name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhpNa5_sud1O",
        "colab_type": "text"
      },
      "source": [
        "Loss Functions\n",
        "The discriminators use MSE loss. The generators use MSE for the adversarial losses and MAE for the cycle consistency losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss1uYj64udIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mean squared error\n",
        "def mse(y_true, y_pred):\n",
        "  loss= tf.reduce_mean(tf.squared_difference(y_pred, y_true))\n",
        "  return loss\n",
        "\n",
        "#mean absolute error\n",
        "def mae(y_true, y_pred):\n",
        "  loss= tf.reduce_mean(tf.abs(y_pred - y_true))\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQgaMfWTvO58",
        "colab_type": "text"
      },
      "source": [
        "Build CycleGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEWAtxfsA_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model= {}\n",
        "\n",
        "#Normalization\n",
        "model['normalization']= InstanceNormalization\n",
        "\n",
        "#optimizers\n",
        "model['opt_D']= Adam(opt['learning_rate_D'], opt['beta_1'], opt['beta_2'])\n",
        "model['opt_G']= Adam(opt['learning_rate_G'], opt['beta_1'], opt['beta_2'])\n",
        "\n",
        "#Build discrimiators\n",
        "D_A= build_discriminator(model, opt, name= 'D_A')\n",
        "D_B= build_discriminator(model, opt, name= 'D_B')\n",
        "\n",
        "#define discrimiantor models\n",
        "image_A= Input(shape= opt['img_shape'])\n",
        "image_B= Input(shape= opt['img_shape'])\n",
        "guess_A= D_A(image_A)\n",
        "guess_B= D_B(image_B)\n",
        "model['D_A']= Model(inputs= image_A, outputs= guess_A, name= 'D_A_model')\n",
        "model['D_B']= Model(inputs= image_B, outputs= guess_B, name= 'D_B_model')\n",
        "\n",
        "#Compile discriminator models\n",
        "loss_weight_D= [0.5]    #0.5 since we train on real and synthetic images\n",
        "model['D_A'].compile(optimizer= model['opt_D'],\n",
        "                     loss= mse,\n",
        "                     loss_weights= loss_weights_D)\n",
        "model['D_B'].compile(optimizer= model['opt_D'],\n",
        "                     loss= mse,\n",
        "                     loss_weights= loss_weights_D)\n",
        "\n",
        "#Use containers to make a static copy of discriminators, used when traiing the generators\n",
        "model['D_A_static']= Network(inputs= image_A, outputs= guess_A, name= 'D_A_static_model')\n",
        "model['D_B_static']= Network(inputs= image_B, outputs= guess_B, name= 'D_B_static_model')\n",
        "\n",
        "#do not update the discriminator weights during the generator training\n",
        "model['D_A_static'].trainable= False\n",
        "model['D_B_static'].trainable= False\n",
        "\n",
        "#Build generators\n",
        "model['G_A2B']= build_generator(model, opt, name= 'G_A2B_model')\n",
        "model['G_B2A']= build_generator(model, opt, name= 'G_B2A_model')\n",
        "\n",
        "#Define full CycleGAN model, used for training the generators\n",
        "real_A= Input(shape=opt['img_shape'], name= 'real_A')\n",
        "real_B= Input(shape= opt['img_shape'], name= 'real_B')\n",
        "synthetic_B= model['G_A2B'](real_A)\n",
        "synthetic_A= model['G_B2A'](real_B)\n",
        "dB_guess_synthetic= model['D_B_static'](synthetic_B)\n",
        "dA_guess_synthetic= model['D_A_static'](synthetic_A)\n",
        "reconstructed_A= model['G_B2A'](synthetic_B)\n",
        "reconstructed_B= model['G_A2B'](synthetic_A)\n",
        "\n",
        "#compile full cycleGAN model\n",
        "model_outputs= [reconstructed_A, reconstructed_B,\n",
        "                dB_guess_synthetic, dA_guess_synthetic]\n",
        "compile_losses= [mae, mae, \n",
        "                 mse, mse]\n",
        "compile_weights= [opt['lambda_ABA'], opt['lambda_BAB'],\n",
        "                  opt['lambda_adversarial'], opt['lambda_adversarial']]\n",
        "model['G_model']= Model(inputs= [real_A, real_B],\n",
        "                        outputs= model_outputs,\n",
        "                        name= 'G_model')\n",
        "model['G_model'].compile(optimizer= model['opt_G'],\n",
        "                         loss= compile_losses,\n",
        "                         loss_weights= compile_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrwN8P4KSvxd",
        "colab_type": "text"
      },
      "source": [
        "Folders and configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l_q8IPeSz4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt['date_time']= time.strrftime('%Y%m%d-%H%M%S',time.localtime()) + '-' + image_folder\n",
        "\n",
        "#Output folder for run data and images\n",
        "opt['out_dir']= os.path.join('image', opt['date_time'])\n",
        "if not os.path.exists(opt['out_dir']):\n",
        "  os.makedirs(opt['out_dir'])\n",
        "\n",
        "#Output folder for saved models\n",
        "if opt['save_models']:\n",
        "  opt['model_out_dir']= os.path.join('saved_models', opt['date_time'])\n",
        "  if not os.path.exists(opt['model_out_dir']):\n",
        "    os.makedirs(opt['model_out_dir'])\n",
        "\n",
        "write_metadata_to_json(model, opt)\n",
        "\n",
        "#Do not pre-allocate GPU memory, allocate as-needed\n",
        "config= tf.ConfigProto()\n",
        "config.gpu_options.allow_growth= True\n",
        "K.tensorflow_backend.set_session(tf.Session(config= config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTZpxa5pUDma",
        "colab_type": "text"
      },
      "source": [
        "#Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFikDrvkUFUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, opt):\n",
        "  \n",
        "  def run_training_batch():\n",
        "\n",
        "    #===================Discriminator Training=====================\n",
        "    #Generate batch of synthetic images\n",
        "    synthetic_images_B= model['G_A2B'].predict(real_images_A)\n",
        "    synthetic_images_A= model['G_B2A'].predict(real_images_B)\n",
        "    synthetic_images_B= synthetic_pool_B.query(synthetic_images_B)\n",
        "    synthetic_images_A= synthetic_pool_A.query(synthetic_images_A)\n",
        "\n",
        "    #Train discriminators on batch\n",
        "    D_loss= []\n",
        "    for _ in range(opt['discriminator_iterations']):\n",
        "      D_A_loss_real= model['D_A'].train_on_batch(x= real_images_A, y= ones)\n",
        "      D_B_loss_real= model['D_B'].train_on_batch(x= real_images_B, y= ones)\n",
        "      D_A_loss_synthetic= model['D_A'].train_on_batch(x= synthetic_images_A, y= zeros)\n",
        "      D_B_loss_synthetic= model['D_B'].train_on_batch(x= synthetic_images_B, y= zeros)\n",
        "      D_A_loss= D_A_loss_real + D_A_loss_synthetic\n",
        "      D_B_loss= D_B_loss_real + D_B_loss_synthetic\n",
        "      D_loss.append(D_A_loss + D_B_loss)\n",
        "\n",
        "    # ====== Generator Training ======\n",
        "    target_data= [real_images_A, real_images_B, ones, ones]\n",
        "\n",
        "    #train generators on batch\n",
        "    G_Loss= []\n",
        "    for _ in range(opt['generator_iterations']):\n",
        "      G_loss.append(model['G_model'].train_on_batch(\n",
        "          x=[real_images_A, real_images_B], y= target_data))\n",
        "      \n",
        "    #=====================================\n",
        "    #update Learning rates\n",
        "    if opt['use_linear_decay'] and epoch >= opt['decay_epoch']:\n",
        "      update_lr(model['D_A'], decay_D)\n",
        "      update_lr(model['D_B'], decay_D)\n",
        "      update_lr(model['G_model'], decay_G)\n",
        "\n",
        "    #Store training losses\n",
        "    D_A_losses.append(D_A_loss)\n",
        "    D_B_losses.append(D_B_loss)\n",
        "    D_losses.append(D_loss[-1])\n",
        "\n",
        "    ABA_reconstruction_loss= G_loss[-1][1]\n",
        "    BAB_reconstruction_loss= G_loss[-1][2]\n",
        "    reconstruction_loss= ABA_reconstruction_loss + BAB_reconstruction_loss\n",
        "    G_AB_adversarial_loss= G_loss[-1][3]\n",
        "    G_BA_adversarial_loss= G_loss[-1][4]\n",
        "\n",
        "    ABA_reconstruction_losses.append(ABA_reconstruction_loss)\n",
        "    BAB_reconstruction_losses.append(BAB_reconstruction_loss)\n",
        "    reconstruction_losses.append(reconstruction_loss)\n",
        "    G_AB_adversarial_losses.append(G_AB_adversarial_loss)\n",
        "    G_BA_adversarial_losses.append(G_BA_adversarial_loss)\n",
        "    G_losses.append9(G_loss[-1][0])\n",
        "\n",
        "    #Print training status\n",
        "    print('\\n')\n",
        "    print('Epoch ----------------------', epoch, '/', opt['epochs'])\n",
        "    print('Loop index -----------------', loop_index + 1, '/', nr_im_per_epoch)\n",
        "    if opt['discriminator_iterations']:\n",
        "      print('Discriminator losses:')\n",
        "      for i in range(opt['discriminator_iterations']):\n",
        "        print('D_loss,' D_loss[i])\n",
        "    if opt['generator_iterations'] > 1:\n",
        "      print('Generator losses:')\n",
        "      for i in range(opt['generator_iterarions']):\n",
        "        print('G_loss', G_loss[i])\n",
        "    print(' Summary:')\n",
        "    print('D_lr', K.get_value(model['D_A'].optimizer.lr))\n",
        "    print('G_lr', K.get_value(model['G_model'].optimizer.lr))\n",
        "    print('D_loss:', D_loss[-1])\n",
        "    print('G_loss:', G_loss[-1][0])\n",
        "    print('reconstruction_loss:', reconstruction_loss)\n",
        "    print_ETA(opt, start_time, epoch, nr_im_per_epoch, loop_index)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if loop_index % 3*opt['batch_size']== 0:\n",
        "      #save temporary images continously\n",
        "      save_tmp_images(model, opt, real_images_A[0], real_images_B[0],\n",
        "                      synthetic_images_A[0], synthetic_images_B[0])\n",
        "      \n",
        "    # ==================================================================\n",
        "    #Begin Training\n",
        "    # ==================================================================\n",
        "    if opt['save_training_img'] and not os.path.exists(os.path.join(opt['out_dir'], 'train_A')):\n",
        "      os.makedirs(os.path.join(opt['out_dir'], 'train_A'))\n",
        "      os.makedirs(os.path.join(opt['out_dir'], 'train_B'))\n",
        "      os.makedirs(os.path.join(opt['out_dir'], 'test_A'))\n",
        "      os.makedirs(os.path.join(opt['out_dir'], 'test_B'))\n",
        "    \n",
        "    D_A_losses= []\n",
        "    D_B_losses= []\n",
        "    D_losses= []\n",
        "\n",
        "    ABA_reconstruction_losses= []\n",
        "    BAB_reconstruction_losses= []\n",
        "    reconstruction_losses= []\n",
        "    G_AB_adversarial_losses= []\n",
        "    G_BA_adversarial_losses= []\n",
        "    G_losses= []\n",
        "\n",
        "\n",
        "    #Image pools used to update the discriminators\n",
        "    synthetic_pool_A= ImagePool(opt['synthetic_pool_size'])\n",
        "    synthetic_pool_B= ImagePool(opt['synthetic_pool_size'])\n",
        "\n",
        "    #Labels used for discriminator training\n",
        "    label_shape= (opt['batch_size'],) + model['D_A'].output_shape[1:]\n",
        "    ones= np.ones(shape= label_shape) * opt['REAL_LABEL']\n",
        "    zeros= ones * 0\n",
        "\n",
        "    #linear learning rate decay\n",
        "    if opt['use_linear_decay']:\n",
        "      decay_D, decay_G= get_lr_linear_rate(opt)\n",
        "\n",
        "    nr_train_im_A= opt['A_train'].shape[0]\n",
        "    nr_train_im_B= opt['B_train'].shape[0]\n",
        "    nr_im_per_epoch= int(np.ceil(np.max((nr_train_im_A, nr_train_im_B)) / opt['batch_size']) * opt['batch_size'])\n",
        "\n",
        "    #Start stopwatch for ETA's\n",
        "    start_time= time.time()\n",
        "    timer_started= False\n",
        "\n",
        "    for epoch in range(1, opt['epochs'] + 1):\n",
        "      #random_order_A= np.random.randint(nr_train_im_A, size= nr_im_per_epoch)\n",
        "      #random_order_B= np.random.randint(nr_train_im_B, size= nr_im_per_epoch)\n",
        "\n",
        "      random_order_A= np.concatenate((np.random.permutation(nr_train_im_A),\n",
        "                                      np.random.randint(nr_train_im_A, size= nr_im_per_epoch - nr_train_im_A)))\n",
        "      random_order_B= np.concatenate((np.random.permutation(nr_train_im_B),\n",
        "                                      np.random.randint(nr_train_im_B, size= nr_im_per_epoch - nr_train_im_B)))\n",
        "      \n",
        "      #Train on image batch\n",
        "      for loop_index in range(0, nr_im_per_epoch, opt['batch_size']):\n",
        "        indices_A= random_order_A[loop_index: loop_index + opt['batch_size']]\n",
        "        indices_B= random_order_B[loop_index: loop_index + opt['batch_size']]\n",
        "\n",
        "        real_images_A= opt['A_train'][indices_A]\n",
        "        real_images_B= opt['B_train'][indices_B]\n",
        "\n",
        "        #Train on image batch\n",
        "        run_training_batch()\n",
        "\n",
        "        #start timer after first (slow) iteration has finished\n",
        "        if not timer_started:\n",
        "          start_time= time.time()\n",
        "          timer_started= True\n",
        "\n",
        "      #Save training image,s\n",
        "      if opt['save_training_img'] and epoch % opt['save_training_img_interval']== 0:\n",
        "        print(' \\n', '\\n', ' ---------------------------- Saving images for epoch', epoch, '---------------------------', '\\n', '\\n')\n",
        "        save_epoch_images(model, opt, epoch)\n",
        "\n",
        "      #Save model\n",
        "      if opt['save_models'] and epoch % 20==0:\n",
        "        save_model(opt, model['D_A'], epoch)\n",
        "        save_model(opt, model['D_B'], epoch)\n",
        "        save_model(opt, model[G_A2B], epoch)\n",
        "        save_model(opt, model['G_B2A'], epoch)\n",
        "\n",
        "\n",
        "      #Save training history\n",
        "      training_history= {\n",
        "          'DA_losses': D_A_losses,\n",
        "          'DB_losses': D_B_losses,\n",
        "          'G_AB_adversarial_losses': G_AB_adversarial_losses,\n",
        "          'G_BA_adversarial_losses': G_Ba_adversarial_losses,\n",
        "          'ABA_reconstruction_losses': ABA_reconstruction_losses,\n",
        "          'BAB_reconstruction_losses': BAB_reconstruction_losses,\n",
        "          'reconstruction_losses': reconstruction_losses,\n",
        "          'D_losses': D_losses,\n",
        "          'G_losses': G_losses}\n",
        "          write_loss_data_to_file(opt, training_history)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}